{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NHC3srjNVKin"
   },
   "source": [
    "In this notebook we will demonstrate aspect based sentiment analysis using [Varder](https://github.com/cjhutto/vaderSentiment) and [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/index.html).<br>\n",
    "<br>**VADER Sentiment Analysis**: VADER (Valence Aware Dictionary and Sentiment Reasoner) is a lexicon and rule-based sentiment analysis tool that is specifically attuned to sentiments expressed in social media, and works well on texts from other domains.(source:[github](https://github.com/cjhutto/vaderSentiment))<br>\n",
    "Stanford NLP have a live demo of aspect based sentiment analysis [here](http://nlp.stanford.edu:8080/sentiment/rntnDemo.html).<br><br>\n",
    "**Stanford Core NLP**: \"Most sentiment prediction systems work just by looking at words in isolation, giving positive points for positive words and negative points for negative words and then summing up these points. That way, the order of words is ignored and important information is lost. In constrast, our new deep learning model actually builds up a representation of whole sentences based on the sentence structure. It computes the sentiment based on how words compose the meaning of longer phrases. This way, the model is not as easily fooled as previous models.\"(source: [Stanford Core NLP](https://nlp.stanford.edu/sentiment/index.html).)\n",
    "\n",
    "Has to be run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 11626,
     "status": "ok",
     "timestamp": 1636346559959,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "rE3_f0TIVKiu",
    "outputId": "a2857c23-57a0-4fd2-8ec0-71dd5a7337d5",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: vaderSentiment in c:\\programdata\\anaconda3\\lib\\site-packages (3.3.2)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from vaderSentiment) (2.25.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2020.12.5)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (1.26.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->vaderSentiment) (2.10)\n",
      "Requirement already satisfied: pycorenlp in c:\\programdata\\anaconda3\\lib\\site-packages (0.3.0)\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from pycorenlp) (2.25.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->pycorenlp) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->pycorenlp) (4.0.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->pycorenlp) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->pycorenlp) (2020.12.5)\n",
      "Requirement already satisfied: wget in c:\\programdata\\anaconda3\\lib\\site-packages (3.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install vaderSentiment\n",
    "!pip install pycorenlp\n",
    "!pip install wget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iedyd0ZHVKiw"
   },
   "source": [
    "### Importing the necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2405,
     "status": "ok",
     "timestamp": 1636346562360,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "o2B3jqrmVKix",
    "outputId": "33ac55a6-584e-4feb-941d-5747a3c46455"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\dtran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\dtran\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.tokenize import word_tokenize, RegexpTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TnHmUUbcVKiy"
   },
   "source": [
    "Lets analyze these three sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "executionInfo": {
     "elapsed": 20,
     "status": "ok",
     "timestamp": 1636346562361,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "5WAU-UJDVKiy"
   },
   "outputs": [],
   "source": [
    "positive = \"This fried chicken tastes very good. It is juicy and perfectly cooked.\"\n",
    "negative = \"This fried chicken tasted bad. It is dry and overcooked.\"\n",
    "ambiguous = \"Except the amazing fried chicken everything else at the restaurant tastes very bad.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z_lwwD_ZVKiz"
   },
   "source": [
    "### VarderSentiment\n",
    "It scores from -1 to 1. -1 being negative and 1 being positive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1636346562361,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "rREISBgBVKi0"
   },
   "outputs": [],
   "source": [
    "def sentiment_analyzer_scores(text):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    score = sentiment_analyzer.polarity_scores(text)\n",
    "    pprint(text)\n",
    "    pprint(score)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1636346562362,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "v4Eb2niZVKi1",
    "outputId": "c4db5b49-5ea7-49de-a4dc-07bfa0b9e276"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive:\n",
      "'This fried chicken tastes very good. It is juicy and perfectly cooked.'\n",
      "{'compound': 0.8122, 'neg': 0.0, 'neu': 0.575, 'pos': 0.425}\n",
      "------------------------------\n",
      "Negative:\n",
      "'This fried chicken tasted bad. It is dry and overcooked.'\n",
      "{'compound': -0.5423, 'neg': 0.28, 'neu': 0.72, 'pos': 0.0}\n",
      "------------------------------\n",
      "Ambiguous:\n",
      "('Except the amazing fried chicken everything else at the restaurant tastes '\n",
      " 'very bad.')\n",
      "{'compound': 0.0018, 'neg': 0.204, 'neu': 0.592, 'pos': 0.204}\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"Positive:\")\n",
    "sentiment_analyzer_scores(positive)\n",
    "\n",
    "print(\"Negative:\")\n",
    "sentiment_analyzer_scores(negative)\n",
    "\n",
    "print(\"Ambiguous:\")\n",
    "sentiment_analyzer_scores(ambiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xWnGxduVKi2"
   },
   "source": [
    "As expected the sentiment analyzer performed well on the positive and negative case. When taking into consideration the ambiguous sentence, it calculated the compound sentiment to be close to 0, i.e, neutral.<br>\n",
    "But it seems to be a negative comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "executionInfo": {
     "elapsed": 16,
     "status": "ok",
     "timestamp": 1636346562362,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "f7yZ6M39VKi2"
   },
   "outputs": [],
   "source": [
    "def get_word_sentiment(text):\n",
    "    sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    tokenized_text = nltk.word_tokenize(text)\n",
    "    \n",
    "    positive_words=[]\n",
    "    neutral_words=[]\n",
    "    negative_words=[]\n",
    "    for word in tokenized_text:\n",
    "        if (sentiment_analyzer.polarity_scores(word)['compound']) >= 0.1:\n",
    "            positive_words.append(word)\n",
    "        elif (sentiment_analyzer.polarity_scores(word)['compound']) <= -0.1:\n",
    "            negative_words.append(word)\n",
    "        else:\n",
    "            neutral_words.append(word)\n",
    "    print(text)\n",
    "    print('Positive:',positive_words)        \n",
    "    print('Negative:',negative_words)    \n",
    "    print('Neutral:',neutral_words)\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 198,
     "status": "ok",
     "timestamp": 1636346562544,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "mDyFFSAmVKi3",
    "outputId": "471ef4cf-e258-4017-f879-615b5aa80fbe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
      "Positive: ['good', 'perfectly']\n",
      "Negative: []\n",
      "Neutral: ['This', 'fried', 'chicken', 'tastes', 'very', '.', 'It', 'is', 'juicy', 'and', 'cooked', '.']\n",
      "------------------------------\n",
      "This fried chicken tasted bad. It is dry and overcooked.\n",
      "Positive: []\n",
      "Negative: ['bad']\n",
      "Neutral: ['This', 'fried', 'chicken', 'tasted', '.', 'It', 'is', 'dry', 'and', 'overcooked', '.']\n",
      "------------------------------\n",
      "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
      "Positive: ['amazing']\n",
      "Negative: ['bad']\n",
      "Neutral: ['Except', 'the', 'fried', 'chicken', 'everything', 'else', 'at', 'the', 'restaurant', 'tastes', 'very', '.']\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_word_sentiment(positive)\n",
    "get_word_sentiment(negative)\n",
    "get_word_sentiment(ambiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TkNg9kKbVKi4"
   },
   "source": [
    "### Stanford Core NLP\n",
    "Before moving on to execute the code we need to start the Stanford Core NLP server on our local machine.<br> To do that follow the steps below (tested on debian should work fine for other distributions too):\n",
    "1. Download the Stanford Core NLP model from [here](https://stanfordnlp.github.io/CoreNLP/#download).\n",
    "2. Unizip the folder\n",
    "3. cd into the folder from the command line<br>\n",
    "    ```cd \"C:\\Users\\dtran\\Google Drive\\Colab Notebooks\\stanford-corenlp-4.3.1\"```\n",
    "4. Start the server using this code from the command line:<br>\n",
    "    ```java -mx5g -cp \"./*\" edu.stanford.nlp.pipeline.StanfordCoreNLPServer -port 8000 -timeout 10000```\n",
    "<br><br>\n",
    "If you do not have java installed on your system please install it from the official [Oracle](https://www.oracle.com/in/java/technologies/javase-downloads.html) page.\n",
    "<br><br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 97930,
     "status": "ok",
     "timestamp": 1636346675054,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "YMCcM9oS2xJv",
    "outputId": "0dd0cc61-53ad-454a-b598-9babc741149c"
   },
   "outputs": [],
   "source": [
    "#get data and unzip\n",
    "!python -m wget https://nlp.stanford.edu/software/stanford-corenlp-latest.zip -o \"stanford-corenlp-latest.zip\"\n",
    "import shutil\n",
    "shutil.unpack_archive('stanford-corenlp-latest.zip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 22425,
     "status": "ok",
     "timestamp": 1636347820231,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "BW0zMnCHQdQ5",
    "outputId": "ba6e02ef-9078-4d85-9941-4e9b84002871"
   },
   "outputs": [],
   "source": [
    "#cd \"C:\\Users\\dtran\\Google Drive\\Colab Notebooks\\stanford-corenlp-4.3.1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycorenlp import StanfordCoreNLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "executionInfo": {
     "elapsed": 158,
     "status": "ok",
     "timestamp": 1636347399564,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "q_rYAUsKVKi4"
   },
   "outputs": [],
   "source": [
    "nlp = StanfordCoreNLP('http://localhost:8000')\n",
    "\n",
    "def get_sentiment(text):\n",
    "    res = nlp.annotate(text,\n",
    "                       properties={'annotators': 'sentiment',\n",
    "                                   'outputFormat': 'json',\n",
    "                                   'timeout': 1000,\n",
    "                       })\n",
    "    print(text)\n",
    "    print('Sentiment:', res['sentences'][0]['sentiment'])\n",
    "    print('Sentiment score:', res['sentences'][0]['sentimentValue'])\n",
    "    print('Sentiment distribution (0-v. negative, 5-v. positive:', res['sentences'][0]['sentimentDistribution'])\n",
    "    print(\"-\"*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 792
    },
    "executionInfo": {
     "elapsed": 196,
     "status": "error",
     "timestamp": 1636347403602,
     "user": {
      "displayName": "dung tran",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64",
      "userId": "14954118940859551276"
     },
     "user_tz": 300
    },
    "id": "36pIyUkdVKi5",
    "outputId": "70d124f9-7797-47e2-b401-fae5627c6f6d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This fried chicken tastes very good. It is juicy and perfectly cooked.\n",
      "Sentiment: Negative\n",
      "Sentiment score: 1\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.12830923698552, 0.37878858949882, 0.30518256344905, 0.17180670417797, 0.01591290588864]\n",
      "------------------------------\n",
      "This fried chicken tasted bad. It is dry and overcooked.\n",
      "Sentiment: Negative\n",
      "Sentiment score: 1\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.35691292388455, 0.38793571113551, 0.18201904294799, 0.04194609175503, 0.03118623027692]\n",
      "------------------------------\n",
      "Except the amazing fried chicken everything else at the restaurant tastes very bad.\n",
      "Sentiment: Negative\n",
      "Sentiment score: 1\n",
      "Sentiment distribution (0-v. negative, 5-v. positive: [0.12830923590495, 0.37878858881094, 0.30518256399302, 0.1718067054989, 0.01591290579219]\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "get_sentiment(positive)\n",
    "get_sentiment(negative)\n",
    "get_sentiment(ambiguous)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E8dRQVXoVKi6"
   },
   "source": [
    "Here you see the model successfully predicts the ambigous sentence which the Varder failed to predict correctly.<br>\n",
    "The code in this notebook has been adapted from this [article](https://towardsdatascience.com/sentiment-analysis-beyond-words-6ca17a6c1b54).See below code for colab."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "01_Aspect_Based_Sentiment_analysis",
   "provenance": [
    {
     "file_id": "https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch9/01_Aspect_Based_Sentiment_analysis.ipynb",
     "timestamp": 1636320999971
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
