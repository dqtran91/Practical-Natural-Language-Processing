{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"},"colab":{"name":"TwitterSentiment_classification.ipynb","provenance":[{"file_id":"https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch8/TwitterSentiment_2.ipynb","timestamp":1636280117976}]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9UOdzgSI5QfD"},"source":["In this notebook we combine the concepts from previous notebook. We pre-process tweets using our preprocessing pipeline, build embeddings and then classify them using a logistic regression model."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"AEcNYi3J5ZJO","executionInfo":{"status":"ok","timestamp":1636280160805,"user_tz":300,"elapsed":4460,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"cdaa94f1-62a3-4fb9-b523-8501b71b799d"},"source":["!wget https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch8/O5_smtd_preprocessing.py -O \"smtd_preprocessing.py\"\n","!wget https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch8/Data/sts_gold_tweet.csv -O \"sts_gold_tweet.csv\"\n","!pip3 install demoji"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-07 10:15:56--  https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch8/O5_smtd_preprocessing.py\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17420 (17K) [text/plain]\n","Saving to: ‘smtd_preprocessing.py’\n","\n","\rsmtd_preprocessing.   0%[                    ]       0  --.-KB/s               \rsmtd_preprocessing. 100%[===================>]  17.01K  --.-KB/s    in 0s      \n","\n","2021-11-07 10:15:56 (36.9 MB/s) - ‘smtd_preprocessing.py’ saved [17420/17420]\n","\n","--2021-11-07 10:15:56--  https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch8/Data/sts_gold_tweet.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 205093 (200K) [text/plain]\n","Saving to: ‘sts_gold_tweet.csv’\n","\n","sts_gold_tweet.csv  100%[===================>] 200.29K  --.-KB/s    in 0.02s   \n","\n","2021-11-07 10:15:56 (12.8 MB/s) - ‘sts_gold_tweet.csv’ saved [205093/205093]\n","\n","Collecting demoji\n","  Downloading demoji-1.1.0-py3-none-any.whl (42 kB)\n","\u001b[K     |████████████████████████████████| 42 kB 1.0 MB/s \n","\u001b[?25hInstalling collected packages: demoji\n","Successfully installed demoji-1.1.0\n"]}]},{"cell_type":"code","metadata":{"id":"bQh8ShMX5QfE"},"source":["#Making the necessary imports\n","import os\n","import sys\n","\n","preprocessing_path = \"smtd_preprocessing.py\"\n","sys.path.append(os.path.abspath(preprocessing_path))\n","\n","import smtd_preprocessing\n","\n","import nltk\n","from nltk.corpus import stopwords\n","nltk.download('stopwords')\n","from string import punctuation\n","\n","import pandas as pd\n","from gensim.models import Word2Vec\n","import warnings\n","warnings.filterwarnings('ignore')\n","\n","from nltk.tokenize import TweetTokenizer\n","tweet_tokenizer = TweetTokenizer()\n","\n","\n","#imports related to modeling\n","import numpy as np\n","from gensim.models import Word2Vec, KeyedVectors\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OU5rhBnv5QfG"},"source":["## Reading and Preprocessing\n","Let's read the dataset and pre-process them using the pre-processing pipeline."]},{"cell_type":"code","metadata":{"id":"tTQIWQfm5QfG","colab":{"base_uri":"https://localhost:8080/","height":485},"executionInfo":{"status":"ok","timestamp":1636280408315,"user_tz":300,"elapsed":5692,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"40d0ef6c-f109-40af-c862-8cba808e4fd1"},"source":["datapath = \"\" #Path to repo\n","df = pd.read_csv(datapath+\"sts_gold_tweet.csv\",\n","                 error_bad_lines=False,delimiter=\";\")\n","df = df.dropna(how='any')\n","df.drop(columns=['id'], inplace=True)\n","display(df.head())\n","\n","#pre-process tweets using our package\n","df['tweet'] = df['tweet'].apply(lambda x: smtd_preprocessing.process_TweetText(x))\n","df['tweet_tokens'] = df['tweet'].apply(lambda x: tweet_tokenizer.tokenize(x))\n","df['tweet_no_stopwords'] = df['tweet_tokens'].apply(lambda x: [word for word in x if word not in stopwords.words('english')])\n","tweets_processed = df['tweet_tokens'].values\n","tweets_cat = df['polarity'].values\n","\n","display(df.head())\n","print(\"Number of tweets and categories\")\n","print(len(tweets_processed), len(tweets_cat))\n","print(\"\\nExamle of polarity, processed tweet, processed tweet without stopwords\")\n","print(tweets_cat[0],',',tweets_processed[0],',',df['tweet_no_stopwords'].values[0])"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>tweet</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>the angel is going to miss the athlete this we...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>It looks as though Shaq is getting traded to C...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@clarianne APRIL 9TH ISN'T COMING SOON ENOUGH</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>drinking a McDonalds coffee and not understand...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>So dissapointed Taylor Swift doesnt have a Twi...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   polarity                                              tweet\n","0         0  the angel is going to miss the athlete this we...\n","1         0  It looks as though Shaq is getting traded to C...\n","2         0     @clarianne APRIL 9TH ISN'T COMING SOON ENOUGH \n","3         0  drinking a McDonalds coffee and not understand...\n","4         0  So dissapointed Taylor Swift doesnt have a Twi..."]},"metadata":{}},{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>polarity</th>\n","      <th>tweet</th>\n","      <th>tweet_tokens</th>\n","      <th>tweet_no_stopwords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>the angel is going to miss the athlete this we...</td>\n","      <td>[the, angel, is, going, to, miss, the, athlete...</td>\n","      <td>[angel, going, miss, athlete, weekend]</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>it looks as though shaq is getting traded to c...</td>\n","      <td>[it, looks, as, though, shaq, is, getting, tra...</td>\n","      <td>[looks, though, shaq, getting, traded, clevela...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>constantnonbrandmention constantdate isn't com...</td>\n","      <td>[constantnonbrandmention, constantdate, isn't,...</td>\n","      <td>[constantnonbrandmention, constantdate, coming...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>drinking a mcdonalds coffee and not understand...</td>\n","      <td>[drinking, a, mcdonalds, coffee, and, not, und...</td>\n","      <td>[drinking, mcdonalds, coffee, understanding, s...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>so dissapointed taylor swift doesnt have a twi...</td>\n","      <td>[so, dissapointed, taylor, swift, doesnt, have...</td>\n","      <td>[dissapointed, taylor, swift, doesnt, twitter]</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   polarity  ...                                 tweet_no_stopwords\n","0         0  ...             [angel, going, miss, athlete, weekend]\n","1         0  ...  [looks, though, shaq, getting, traded, clevela...\n","2         0  ...  [constantnonbrandmention, constantdate, coming...\n","3         0  ...  [drinking, mcdonalds, coffee, understanding, s...\n","4         0  ...     [dissapointed, taylor, swift, doesnt, twitter]\n","\n","[5 rows x 4 columns]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Number of tweets and categories\n","2034 2034\n","\n","Examle of polarity, processed tweet, processed tweet without stopwords\n","0 , ['the', 'angel', 'is', 'going', 'to', 'miss', 'the', 'athlete', 'this', 'weekend'] , ['angel', 'going', 'miss', 'athlete', 'weekend']\n"]}]},{"cell_type":"markdown","metadata":{"id":"HS8Y3RHx5QfH"},"source":["## Train your own Embedding"]},{"cell_type":"code","metadata":{"id":"i0N5Bo-P5QfH","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636280592511,"user_tz":300,"elapsed":417,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"2e7c0d48-67be-43ef-b66f-f7cd3600d581"},"source":["#CBOW\n","import time\n","start = time.time()\n","w2v_model = Word2Vec(tweets_processed,min_count=5, sg=0)\n","end = time.time()\n","\n","print(\"CBOW Model Training Complete.\\nTime taken for training is:{:.5f} sec \".format((end-start)))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CBOW Model Training Complete.\n","Time taken for training is:0.32860 sec \n"]}]},{"cell_type":"code","metadata":{"id":"6v7UnO2n5QfI","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636280594617,"user_tz":300,"elapsed":388,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"a4caeed2-22d8-4933-8340-9bf052bdcec2"},"source":["#Create document vectors by averaging word vectors.\n","def embedding_feats(list_of_lists):\n","    DIMENSION = 100\n","    zero_vector = np.zeros(DIMENSION)\n","    feats = []\n","    for tokens in list_of_lists:\n","        feat_for_this =  np.zeros(DIMENSION)\n","        count_for_this = 0\n","        for token in tokens:\n","            if token in w2v_model:\n","                feat_for_this += w2v_model[token]\n","                count_for_this +=1\n","        feats.append(feat_for_this/count_for_this if count_for_this > 0 else feat_for_this)        \n","    return feats\n","\n","train_vectors = embedding_feats(df['tweet_no_stopwords'].values)\n","print(len(train_vectors))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2034\n"]}]},{"cell_type":"code","metadata":{"id":"hR5-Wcse5QfJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636280605680,"user_tz":300,"elapsed":159,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"7c178e45-144b-429d-f6d1-76c138c32fdf"},"source":["#Take any classifier (LogisticRegression here)\n","classifier = LogisticRegression(random_state=2020)\n","train_data, test_data, train_cats, test_cats = train_test_split(train_vectors, \n","                                                                df['polarity'].values)\n","\n","classifier.fit(train_data, train_cats)\n","print(\"Accuracy: \", classifier.score(test_data, test_cats))\n","preds = classifier.predict(test_data)\n","print(classification_report(test_cats, preds))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Accuracy:  0.6758349705304518\n","              precision    recall  f1-score   support\n","\n","           0       0.68      1.00      0.81       344\n","           4       0.00      0.00      0.00       165\n","\n","    accuracy                           0.68       509\n","   macro avg       0.34      0.50      0.40       509\n","weighted avg       0.46      0.68      0.55       509\n","\n"]}]}]}