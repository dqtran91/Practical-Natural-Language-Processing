{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"02_Bag_of_Words.ipynb","provenance":[{"file_id":"https://github.com/practical-nlp/practical-nlp/blob/master/Ch3/02_Bag_of_Words.ipynb","timestamp":1612482938922}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.7"}},"cells":[{"cell_type":"markdown","metadata":{"id":"bMoADcrhJP2H"},"source":["## Bag of Words\n","\n","In the last notebook, we saw how to get the one hot encoding representation for our toy corpus. In this notebook we will see how to use bag of words representation for the same data.."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mhnX4sORJP2J","executionInfo":{"status":"ok","timestamp":1613654215701,"user_tz":300,"elapsed":393,"user":{"displayName":"dung tran","photoUrl":"https://lh4.googleusercontent.com/-doHYXWVx74Y/AAAAAAAAAAI/AAAAAAAAQIU/3POXOLrewRE/s64/photo.jpg","userId":"14954118940859551276"}},"outputId":"09daccaa-f729-4f8b-eb5e-f0d6fd31bcb3"},"source":["documents = [\"Dog bites man.\", \"Man bites dog.\", \"Dog eats meat.\", \"Man eats food.\"] #Same as the earlier notebook\n","processed_docs = [doc.lower().replace(\".\",\"\") for doc in documents]\n","processed_docs"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"zEm3kuokJP2N"},"source":["Now, let's do the main task of finding bag of words representation. We will use CountVectorizer from sklearn. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pbCdQVWQJP2O","scrolled":true,"executionInfo":{"status":"ok","timestamp":1613654858600,"user_tz":300,"elapsed":1490,"user":{"displayName":"dung tran","photoUrl":"https://lh4.googleusercontent.com/-doHYXWVx74Y/AAAAAAAAAAI/AAAAAAAAQIU/3POXOLrewRE/s64/photo.jpg","userId":"14954118940859551276"}},"outputId":"19ac97b4-3342-405f-e935-573766e75960"},"source":["from sklearn.feature_extraction.text import CountVectorizer\n","\n","#look at the documents list\n","print(\"Our corpus: \", processed_docs)\n","\n","count_vect = CountVectorizer()\n","#Build a BOW representation for the corpus\n","bow_rep = count_vect.fit_transform(processed_docs)\n","\n","#Look at the vocabulary mapping\n","print(\"Our vocabulary: \", count_vect.vocabulary_)\n","\n","#see the BOW rep for first 2 documents\n","print(\"BoW representation for 'dog bites man': \", bow_rep[0].toarray())\n","print(\"BoW representation for 'man bites dog: \",bow_rep[1].toarray())\n","\n","#Get the representation using this vocabulary, for a new text\n","temp = count_vect.transform([\"dog and dog are friends\"])\n","print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Our corpus:  ['dog bites man', 'man bites dog', 'dog eats meat', 'man eats food']\n","Our vocabulary:  {'dog': 1, 'bites': 0, 'man': 4, 'eats': 2, 'meat': 5, 'food': 3}\n","BoW representation for 'dog bites man':  [[1 1 0 0 1 0]]\n","BoW representation for 'man bites dog:  [[1 1 0 0 1 0]]\n","Bow representation for 'dog and dog are friends': [[0 2 0 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"FUPbDqLReOQ_"},"source":["In the above code, we represented the text considering the frequency of words into account. However, sometimes, we don't care about frequency much, but only want to know whether a word appeared in a text or not. That is, each document is represented as a vector of 0s and 1s. We will use the option binary=True in CountVectorizer for this purpose. "]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bvFoqDRAJP2Q","executionInfo":{"status":"ok","timestamp":1612624976832,"user_tz":300,"elapsed":303,"user":{"displayName":"dung tran","photoUrl":"https://lh4.googleusercontent.com/-doHYXWVx74Y/AAAAAAAAAAI/AAAAAAAAQIU/3POXOLrewRE/s64/photo.jpg","userId":"14954118940859551276"}},"outputId":"b0bab732-88a9-41ea-e1f6-6942b6989ff2"},"source":["#BoW with binary vectors\n","count_vect = CountVectorizer(binary=True)\n","bow_rep_bin = count_vect.fit_transform(processed_docs)\n","temp = count_vect.transform([\"dog and dog are friends\"])\n","print(\"Bow representation for 'dog and dog are friends':\", temp.toarray())"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Bow representation for 'dog and dog are friends': [[0 1 0 0 0 0]]\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"04eIThgleORA"},"source":["We will see how we can use BoW representation for Text Classification later in Chapter 4. "]}]}