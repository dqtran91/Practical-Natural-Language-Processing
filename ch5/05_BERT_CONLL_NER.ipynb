{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"05_BERT_CONLL_NER.ipynb","provenance":[{"file_id":"https://github.com/practical-nlp/practical-nlp/blob/master/Ch5/05_BERT_CONLL_NER.ipynb","timestamp":1620342120063}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.10"}},"cells":[{"cell_type":"markdown","metadata":{"id":"SFcnXxiQX-bS"},"source":["In this notebook we demonstrate how we can leverage BERT to perform NER on conll2003 dataset.<br>\n","This notebook requires a GPU to get setup. We suggest you to run this on your local machine only if you have a GPU setup or else you can use google colab."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZNKhShOaylVt","executionInfo":{"status":"ok","timestamp":1639365416770,"user_tz":300,"elapsed":24230,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"e0294684-99f2-456f-feef-ee8bd78ee560"},"source":["#Installing required packages \n","# %tensorflow_version 1.x\n","!pip install pytorch-pretrained-bert\n","!pip install seqeval\n","\n","#importing packages for string processing,dataframe handling, array manipulations, etc\n","import string\n","import pandas as pd\n","import numpy as np\n","from tqdm import tqdm, trange\n","\n","#importing all the pytorch packages\n","import torch\n","from torch.optim import Adam\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertForTokenClassification, BertAdam\n","\n","#importing additonal packages to aid preprocessing of data\n","from keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","\n","#importing packages to calculate the f1_score of our model\n","from seqeval.metrics import f1_score\n","\n"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-pretrained-bert\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[?25l\r\u001b[K     |██▋                             | 10 kB 21.5 MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 20 kB 25.8 MB/s eta 0:00:01\r\u001b[K     |████████                        | 30 kB 23.6 MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 40 kB 18.7 MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 51 kB 15.7 MB/s eta 0:00:01\r\u001b[K     |███████████████▉                | 61 kB 10.9 MB/s eta 0:00:01\r\u001b[K     |██████████████████▌             | 71 kB 11.8 MB/s eta 0:00:01\r\u001b[K     |█████████████████████▏          | 81 kB 12.8 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▉        | 92 kB 13.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 102 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 112 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▊| 122 kB 12.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 123 kB 12.6 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2.23.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.19.5)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (2019.12.20)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (1.10.0+cu111)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert) (4.62.3)\n","Collecting boto3\n","  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 42.9 MB/s \n","\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert) (3.10.0.2)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.24.0,>=1.23.23\n","  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n","\u001b[K     |████████████████████████████████| 8.4 MB 30.4 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.3 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 44.3 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-pretrained-bert) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3->pytorch-pretrained-bert) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 40.1 MB/s \n","\u001b[?25hRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert) (2.10)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, boto3, pytorch-pretrained-bert\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 pytorch-pretrained-bert-0.6.2 s3transfer-0.5.0 urllib3-1.25.11\n","Collecting seqeval\n","  Downloading seqeval-1.2.2.tar.gz (43 kB)\n","\u001b[K     |████████████████████████████████| 43 kB 1.7 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.19.5)\n","Requirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.7/dist-packages (from seqeval) (1.0.1)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.0.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.1.0)\n","Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.4.1)\n","Building wheels for collected packages: seqeval\n","  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for seqeval: filename=seqeval-1.2.2-py3-none-any.whl size=16181 sha256=f7c0f368e48dc4ef535a80b4a9724697f202e5288eef0990fa899892e34b21c3\n","  Stored in directory: /root/.cache/pip/wheels/05/96/ee/7cac4e74f3b19e3158dce26a20a1c86b3533c43ec72a549fd7\n","Successfully built seqeval\n","Installing collected packages: seqeval\n","Successfully installed seqeval-1.2.2\n"]}]},{"cell_type":"code","metadata":{"id":"hVfxrl-HOkTn","executionInfo":{"status":"ok","timestamp":1639365416770,"user_tz":300,"elapsed":11,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["# #uploading data into google colab\n","# #upload the test.txt and train.txt files respectively\n","# from google.colab import files\n","# uploaded = files.upload()\n","\n"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hUhTgqFV7l2g","executionInfo":{"status":"ok","timestamp":1639365417755,"user_tz":300,"elapsed":996,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"09eb6803-9cb6-4cc7-fece-b505c58da5ea"},"source":["!wget -P DataFolder https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/train.txt\n","!wget -P DataFolder https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/test.txt"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-13 03:16:56--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/train.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.110.133, 185.199.108.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 1655711 (1.6M) [text/plain]\n","Saving to: ‘DataFolder/train.txt’\n","\n","train.txt           100%[===================>]   1.58M  --.-KB/s    in 0.03s   \n","\n","2021-12-13 03:16:56 (62.8 MB/s) - ‘DataFolder/train.txt’ saved [1655711/1655711]\n","\n","--2021-12-13 03:16:56--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conlldata/test.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 376236 (367K) [text/plain]\n","Saving to: ‘DataFolder/test.txt’\n","\n","test.txt            100%[===================>] 367.42K  --.-KB/s    in 0.01s   \n","\n","2021-12-13 03:16:57 (24.2 MB/s) - ‘DataFolder/test.txt’ saved [376236/376236]\n","\n"]}]},{"cell_type":"code","metadata":{"id":"HkbURKk5y0_1","executionInfo":{"status":"ok","timestamp":1639365417755,"user_tz":300,"elapsed":4,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["\"\"\"\n","Load the training/testing data. \n","input: conll format data, but with only 2 tab separated colums - words and NEtags.\n","output: A list where each item is 2 lists.  sentence as a list of tokens, NER tags as a list for each token.\n","\"\"\"\n","#functions for preparing the data in the *.txt files\n","def load__data_conll(file_path):\n","    myoutput,words,tags = [],[],[]\n","    fh = open(file_path)\n","    for line in fh:\n","        line = line.strip()\n","        if \"\\t\" not in line:\n","            #Sentence ended.\n","            myoutput.append([words,tags])\n","            words,tags = [],[]\n","        else:\n","            word, tag = line.split(\"\\t\")\n","            words.append(word)\n","            tags.append(tag)\n","    fh.close()\n","    return myoutput\n","\n","#preprocess the data by calling the functions\n","train_path = 'DataFolder/train.txt'\n","test_path = 'DataFolder/test.txt'\n","conll_train = load__data_conll(train_path)\n","conll_test = load__data_conll(test_path)    "],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oYm4B66pBN6T","executionInfo":{"status":"ok","timestamp":1639365418482,"user_tz":300,"elapsed":482,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"54f6c1a8-271d-4146-99dc-a1a3ac5dced5"},"source":["#BERT needs us to pre-process the data in a particular way.\n","#Lets take the raw data from the txt files\n","df_train = pd.read_csv(\"DataFolder/train.txt\", engine=\"python\",delimiter=\"\\t\",header=None,encoding='utf-8',error_bad_lines=False)\n","df_test = pd.read_csv(\"DataFolder/test.txt\", engine=\"python\",delimiter=\"\\t\",encoding='utf-8',header=None, error_bad_lines=False)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stderr","text":["Skipping line 23407: unexpected end of data\n"]}]},{"cell_type":"code","metadata":{"id":"tQS3WdwLBOJe","executionInfo":{"status":"ok","timestamp":1639365419481,"user_tz":300,"elapsed":1001,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#merge \n","df = pd.merge(df_train,df_test)\n","label = list(df[1].values)#we will be using this to make a set of all unique labels"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"av7yRSvrX4-z","executionInfo":{"status":"ok","timestamp":1639365419482,"user_tz":300,"elapsed":13,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"6d2d8e86-337b-410f-e903-2fb4634ed277"},"source":["np.array(conll_train).shape#calculating the size"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["(14041, 2)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3BGxbUspYAPU","executionInfo":{"status":"ok","timestamp":1639365419482,"user_tz":300,"elapsed":11,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"278014dd-7c9a-4396-e0c0-62d387156f9f"},"source":["np.array(conll_test).shape#calculating the size"],"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:1: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n","  \"\"\"Entry point for launching an IPython kernel.\n"]},{"output_type":"execute_result","data":{"text/plain":["(3453, 2)"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","metadata":{"id":"hBA0ccq2YUGN"},"source":["We need to join all the tokens into a single sentence. We will use the untokenize function in token_utils from [this](https://github.com/commonsense/metanl/blob/master/metanl/token_utils.py) github repo.\n"]},{"cell_type":"code","metadata":{"id":"3046zKhaTXOW","executionInfo":{"status":"ok","timestamp":1639365419483,"user_tz":300,"elapsed":9,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["import re\n","def untokenize(words):\n","    \"\"\"\n","    Untokenizing a text undoes the tokenizing operation, restoring\n","    punctuation and spaces to the places that people expect them to be.\n","    Ideally, `untokenize(tokenize(text))` should be identical to `text`,\n","    except for line breaks.\n","    \"\"\"\n","    text = ' '.join(words)\n","    step1 = text.replace(\"`` \", '\"').replace(\" ''\", '\"').replace('. . .',  '...')\n","    step2 = step1.replace(\" ( \", \" (\").replace(\" ) \", \") \")\n","    step3 = re.sub(r' ([.,:;?!%]+)([ \\'\"`])', r\"\\1\\2\", step2)\n","    step4 = re.sub(r' ([.,:;?!%]+)$', r\"\\1\", step3)\n","    step5 = step4.replace(\" '\", \"'\").replace(\" n't\", \"n't\").replace(\n","         \"can not\", \"cannot\")\n","    step6 = step5.replace(\" ` \", \" '\")\n","    return step6.strip()\n"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nvtQJ1Fxa89j"},"source":["Lets start with pre-processing the data for BERT"]},{"cell_type":"code","metadata":{"id":"uEM8-XuPYeWt","executionInfo":{"status":"ok","timestamp":1639365419483,"user_tz":300,"elapsed":9,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#lets convert them to dataframs for easier handling\n","df_train = pd.DataFrame(conll_train,columns=[\"sentence\",\"labels\"])\n","df_test = pd.DataFrame(conll_test,columns=[\"sentence\",\"labels\"])"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5WgHCbX_ZAqG","executionInfo":{"status":"ok","timestamp":1639365419483,"user_tz":300,"elapsed":8,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"8a3e8201-50aa-4958-bd29-356a78e422c3"},"source":["#getting all the sentences and labels present in both test and train\n","sentences = list(df_train['sentence'])+list(df_test['sentence'])\n","print(\"No of sentences:\",len(sentences))\n","labels = list(df_train['labels'])+list(df_test['labels']) \n","print(\"No of labels:\",len(labels))"],"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["No of sentences: 17494\n","No of labels: 17494\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"_KW3zKy7ZAnB","executionInfo":{"status":"ok","timestamp":1639365419704,"user_tz":300,"elapsed":227,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"fc1b97ee-2cd6-4dd1-9548-91eb0b815046"},"source":["sentences = [untokenize(sent) for sent in sentences]\n","sentences[0]"],"execution_count":12,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'EU rejects German call to boycott British lamb.'"]},"metadata":{},"execution_count":12}]},{"cell_type":"markdown","metadata":{"id":"W88K8YVXh7QA"},"source":["We need to now tokenize the sentences and then add the CLS and SEP tokens as BERT expects the input in such a format."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3JUPhdRFzea1","executionInfo":{"status":"ok","timestamp":1639365425955,"user_tz":300,"elapsed":6258,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"78cbc160-8133-4903-9d18-5d0cf267a133"},"source":["#setting up pytorch to use GPU\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","\n","#prescribed configurations that we need to fix for BERT.\n","MAX_LEN = 75\n","bs = 32\n","\n","#BERT's implementation comes with a pretained tokenizer and a defined vocabulary\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","\n","#tokenizing the text \n","tokenized_texts = list(map(lambda x: ['[CLS]'] + tokenizer.tokenize(x) + ['[SEP]'] , sentences))\n","print(tokenized_texts[0])"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 231508/231508 [00:00<00:00, 700987.53B/s]\n"]},{"output_type":"stream","name":"stdout","text":["['[CLS]', 'eu', 'rejects', 'german', 'call', 'to', 'boycott', 'british', 'lamb', '.', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Niy1e_tU7F0c","executionInfo":{"status":"ok","timestamp":1639365425956,"user_tz":300,"elapsed":12,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"6af38eae-09a2-4f2c-ca2e-7f3fb05de11f"},"source":["labels[0]"],"execution_count":14,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['B-ORG', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O']"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","metadata":{"id":"7xxx7mF28NZi","executionInfo":{"status":"ok","timestamp":1639365426330,"user_tz":300,"elapsed":7,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#pre-processing the labels\n","#converting tags to indices \n","tags_vals = list(set(label))  \n","tag2idx = {t: i for i, t in enumerate(tags_vals)}"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"mvUnUjvk8sOB","executionInfo":{"status":"ok","timestamp":1639365426806,"user_tz":300,"elapsed":482,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#We now need to give BERT input ids,ie, a sequence of integers which uniquely identify each input token to its index number.\n","#cutting and padding the tokens and labels to our desired length\n","\n","input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","tags = pad_sequences([[tag2idx.get(l) for l in lab] for lab in labels],\n","                     maxlen=MAX_LEN, value=tag2idx[\"O\"], padding=\"post\",\n","                     dtype=\"long\", truncating=\"post\")"],"execution_count":16,"outputs":[]},{"cell_type":"code","metadata":{"id":"tDb-q3HXEumu","executionInfo":{"status":"ok","timestamp":1639365427651,"user_tz":300,"elapsed":846,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#BERT supports something called attention masks\n","#Tells the model which tokens should be attended to, and which should not.\n","#learn more about this at https://huggingface.co/transformers/glossary.html#attention-mask\n","\n","attention_masks = [[float(i>0) for i in ii] for ii in input_ids]"],"execution_count":17,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"drV54DRsbYPc"},"source":["Now we need to split the data into train and validation. Convert it to tensors and then create an iterator for our data"]},{"cell_type":"code","metadata":{"id":"_LuJ9LeDEwex","executionInfo":{"status":"ok","timestamp":1639365427652,"user_tz":300,"elapsed":5,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#split the dataset to use 20% to validate the model.\n","tr_inputs, val_inputs, tr_tags, val_tags = train_test_split(input_ids, tags, \n","                                                            random_state=2020, test_size=0.2)\n","tr_masks, val_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2020, test_size=0.2)"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"KuS8WeQyEyG6","executionInfo":{"status":"ok","timestamp":1639365427652,"user_tz":300,"elapsed":5,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#pytorch requires inputs to be in the form of torch tensors\n","#Learn more about torch tensors at https://pytorch.org/docs/stable/tensors.html\n","tr_inputs = torch.tensor(tr_inputs)\n","val_inputs = torch.tensor(val_inputs)\n","tr_tags = torch.tensor(tr_tags)\n","val_tags = torch.tensor(val_tags)\n","tr_masks = torch.tensor(tr_masks)\n","val_masks = torch.tensor(val_masks)"],"execution_count":19,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u-YtGlTXEztA","executionInfo":{"status":"ok","timestamp":1639365427652,"user_tz":300,"elapsed":4,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"8ae0054b-d658-49c6-e11a-0dd46c05de29"},"source":["#Define the Data Loaders\n","#Shuffle the data at training time\n","#Pass them sequentially during test time\n","train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=bs)\n","print(\"Train Data Loaders Ready\")\n","valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n","valid_sampler = SequentialSampler(valid_data)\n","valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=bs)\n","print(\"Test Data Loaders Ready\")"],"execution_count":20,"outputs":[{"output_type":"stream","name":"stdout","text":["Train Data Loaders Ready\n","Test Data Loaders Ready\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ajuboku7E1Y0","executionInfo":{"status":"ok","timestamp":1639365452681,"user_tz":300,"elapsed":25032,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"b038a03c-1e83-49b6-8f8e-109358c10658"},"source":["# BertForTokenClassification class of pytorch-pretrained-bert package provides  for token-level predictions\n","model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))#loading pre trained bert\n","print(\"BERT model ready to use\")"],"execution_count":21,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 407873900/407873900 [00:14<00:00, 28844011.20B/s]\n"]},{"output_type":"stream","name":"stdout","text":["BERT model ready to use\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"En7S5Jj2E3Sh","executionInfo":{"status":"ok","timestamp":1639365463763,"user_tz":300,"elapsed":11084,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"7bf99d0d-746a-4614-d4c1-4be6af158744"},"source":["#Passing model parameters into GPU\n","print(\"Passing Model parameters in GPU\")\n","model.cuda()"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Passing Model parameters in GPU\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForTokenClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): BertLayerNorm()\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): BertLayerNorm()\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): BertLayerNorm()\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":22}]},{"cell_type":"markdown","metadata":{"id":"3dlWDBc1az7H"},"source":["Finally, we move to fine  tuning BERT"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"it5l2g-cE439","executionInfo":{"status":"ok","timestamp":1639365463764,"user_tz":300,"elapsed":17,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"3f91b545-cecb-4476-f4c1-c80640ecefb8"},"source":["#Before starting fine tuing we need to add the optimizer. Generally Adam is used\n","#weight_decay is added as regularization to the main weight matrices\n","print(\"Fine Tuning BERT\")\n","FULL_FINETUNING = True\n","if FULL_FINETUNING:\n","    param_optimizer = list(model.named_parameters())\n","    no_decay = ['bias', 'gamma', 'beta']\n","    optimizer_grouped_parameters = [\n","        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.01},\n","        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","         'weight_decay_rate': 0.0}\n","    ]\n","else:\n","    param_optimizer = list(model.classifier.named_parameters()) \n","    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n","optimizer = Adam(optimizer_grouped_parameters, lr=3e-5)"],"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["Fine Tuning BERT\n"]}]},{"cell_type":"code","metadata":{"id":"w7jNHHs1E6u1","executionInfo":{"status":"ok","timestamp":1639365463764,"user_tz":300,"elapsed":11,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["#accuracy\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=2).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":415},"id":"nZHdynvRFDv1","executionInfo":{"status":"error","timestamp":1639365933901,"user_tz":300,"elapsed":470147,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"7308d937-5ef7-4865-bde6-492b3f2a2eb8"},"source":["#Add the epoch number. The bert paper recomends 3-4\n","epochs = 4\n","max_grad_norm = 1.0\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","n_gpu = torch.cuda.device_count()\n","train_loss_set=[]\n","for _ in trange(epochs, desc=\"Epoch\"):\n","    # TRAIN loop\n","    model.train()\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    for step, batch in enumerate(train_dataloader):\n","        # add batch to gpu\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # forward pass\n","        loss = model(b_input_ids, token_type_ids=None,\n","                     attention_mask=b_input_mask, labels=b_labels)\n","        train_loss_set.append(loss)\n","        # backward pass\n","        loss.backward()\n","        # track train loss\n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(parameters=model.parameters(), max_norm=max_grad_norm)\n","        # update parameters\n","        optimizer.step()\n","        model.zero_grad()\n","    # print train loss per epoch\n","    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","    # VALIDATION on validation set\n","    model.eval()\n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    predictions , true_labels = [], []\n","    for batch in valid_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        b_input_ids, b_input_mask, b_labels = batch\n","        \n","        with torch.no_grad():\n","            tmp_eval_loss = model(b_input_ids, token_type_ids=None,\n","                                  attention_mask=b_input_mask, labels=b_labels)\n","            logits = model(b_input_ids, token_type_ids=None,\n","                           attention_mask=b_input_mask)\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        predictions.extend([list(p) for p in np.argmax(logits, axis=2)])\n","        true_labels.append(label_ids)\n","        \n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n","        \n","        eval_loss += tmp_eval_loss.mean().item()\n","        eval_accuracy += tmp_eval_accuracy\n","        \n","        nb_eval_examples += b_input_ids.size(0)\n","        nb_eval_steps += 1\n","    eval_loss = eval_loss/nb_eval_steps\n","    print(\"Validation loss: {}\".format(eval_loss))\n","    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","    pred_tags = [tags_vals[p_i] for p in predictions for p_i in p]\n","    valid_tags = [tags_vals[l_ii] for l in true_labels for l_i in l for l_ii in l_i]\n","    print(\"F1-Score: {}\".format(f1_score(pred_tags, valid_tags)))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/4 [00:00<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Train loss: 0.3073566857912497\n"]},{"output_type":"stream","name":"stderr","text":["\rEpoch:   0%|          | 0/4 [07:49<?, ?it/s]"]},{"output_type":"stream","name":"stdout","text":["Validation loss: 0.18993846184828064\n","Validation Accuracy: 0.9339573002754822\n"]},{"output_type":"stream","name":"stderr","text":["\n"]},{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-ffe379bab800>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mpred_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtags_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mp_i\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mvalid_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtags_vals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0ml_ii\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrue_labels\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_i\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ml_ii\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ml_i\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"F1-Score: {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpred_tags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mf1_score\u001b[0;34m(y_true, y_pred, average, suffix, mode, sample_weight, zero_division, scheme)\u001b[0m\n\u001b[1;32m    363\u001b[0m                                                      \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m                                                      \u001b[0mzero_division\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mzero_division\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m                                                      suffix=suffix)\n\u001b[0m\u001b[1;32m    366\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/sequence_labeling.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, suffix)\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mscheme\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         \u001b[0msuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m         \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    140\u001b[0m     )\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36m_precision_recall_fscore_support\u001b[0;34m(y_true, y_pred, average, warn_for, beta, sample_weight, zero_division, scheme, suffix, extract_tp_actual_correct)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'average has to be one of {}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maverage_options\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m     \u001b[0mpred_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtp_sum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mextract_tp_actual_correct\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msuffix\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscheme\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/seqeval/metrics/v1.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mis_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_list\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Found input variables without list of list.'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mlen_true\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen_pred\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Found input variables without list of list."]}]}]}