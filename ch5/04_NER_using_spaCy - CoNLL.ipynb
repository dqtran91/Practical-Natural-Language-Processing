{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"04_NER_using_spaCy - CoNLL.ipynb","provenance":[{"file_id":"https://github.com/practical-nlp/practical-nlp/blob/master/Ch5/04_NER_using_spaCy%20-%20CoNLL.ipynb","timestamp":1620340820403}]},"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"language_info":{"codemirror_mode":{"name":"ipython","version":2},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython2","version":"2.7.17"}},"cells":[{"cell_type":"markdown","metadata":{"id":"q39927UZ1pSH"},"source":["# Training and Evaluating an NER model with spaCy on the CoNLL dataset\n","\n","In this notebook, we will take a look at using spaCy commandline to train and evaluate a NER model. We will also compare it with the pretrained NER model in spacy. \n","\n","Note: we will create multiple folders during this experiment:\n","spacyNER_data "]},{"cell_type":"markdown","metadata":{"id":"PnbsGLWN1pSN"},"source":["## Step 1: Converting data to json structures so it can be used by Spacy"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RyXbqhgN3E3Z","executionInfo":{"status":"ok","timestamp":1635985032532,"user_tz":240,"elapsed":2282,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"7f88d32e-d37e-4b92-a90b-3faf29bf3a1e"},"source":["!wget -P Data https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/test.txt\n","!wget -P Data https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/train.txt\n","!wget -P Data https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/valid.txt    "],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-11-04 00:17:11--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/test.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 748095 (731K) [text/plain]\n","Saving to: ‘Data/test.txt’\n","\n","test.txt            100%[===================>] 730.56K  --.-KB/s    in 0.08s   \n","\n","2021-11-04 00:17:11 (9.36 MB/s) - ‘Data/test.txt’ saved [748095/748095]\n","\n","--2021-11-04 00:17:12--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/train.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 3283420 (3.1M) [text/plain]\n","Saving to: ‘Data/train.txt’\n","\n","train.txt           100%[===================>]   3.13M  --.-KB/s    in 0.1s    \n","\n","2021-11-04 00:17:12 (29.3 MB/s) - ‘Data/train.txt’ saved [3283420/3283420]\n","\n","--2021-11-04 00:17:12--  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch5/Data/conll2003/en/valid.txt\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.111.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 827443 (808K) [text/plain]\n","Saving to: ‘Data/valid.txt’\n","\n","valid.txt           100%[===================>] 808.05K  --.-KB/s    in 0.07s   \n","\n","2021-11-04 00:17:13 (11.0 MB/s) - ‘Data/valid.txt’ saved [827443/827443]\n","\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Xo--wCER1pSO","executionInfo":{"status":"ok","timestamp":1635985036730,"user_tz":240,"elapsed":4208,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"ed237fec-121a-4f3d-e43d-8ed93bed3fad"},"source":["#Read the CONLL data from conll2003 folder, and store the formatted data into a folder spacyNER_data\n","!mkdir spacyNER_data\n","#the above two lines create folders if they don't exist. If they do, the output shows a message that it\n","#already exists and cannot be created again\n","!python3 -m spacy convert \"Data/train.txt\" spacyNER_data -c ner\n","!python3 -m spacy convert \"Data/test.txt\" spacyNER_data -c ner\n","!python3 -m spacy convert \"Data/valid.txt\" spacyNER_data -c ner"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n","\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n","\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n","into documents with `-n 10`.\u001b[0m\n","\u001b[38;5;2m✔ Generated output file (14987 documents): spacyNER_data/train.json\u001b[0m\n","\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n","\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n","\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n","into documents with `-n 10`.\u001b[0m\n","\u001b[38;5;2m✔ Generated output file (3684 documents): spacyNER_data/test.json\u001b[0m\n","\u001b[38;5;4mℹ Auto-detected token-per-line NER format\u001b[0m\n","\u001b[38;5;4mℹ Grouping every 1 sentences into a document.\u001b[0m\n","\u001b[38;5;3m⚠ To generate better training data, you may want to group sentences\n","into documents with `-n 10`.\u001b[0m\n","\u001b[38;5;2m✔ Generated output file (3466 documents): spacyNER_data/valid.json\u001b[0m\n"]}]},{"cell_type":"markdown","metadata":{"id":"dE6MAFKe1pSP"},"source":["#### For example, the data before and after running spacy's convert program looks as follows."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wTTtRyJ1pSP","scrolled":true,"executionInfo":{"status":"ok","timestamp":1635985037945,"user_tz":240,"elapsed":1226,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"86d3ff1f-d765-4743-90e3-61894a4a1ab7"},"source":["!echo \"BEFORE : (Data/train.txt)\"\n","!head \"Data/train.txt\" -n 11 | tail -n 9\n","!echo \"\\nAFTER : (Data/train.json)\"\n","!head \"spacyNER_data/train.json\" -n 64 | tail -n 49"],"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["BEFORE : (Data/train.txt)\n","EU NNP B-NP B-ORG\n","rejects VBZ B-VP O\n","German JJ B-NP B-MISC\n","call NN I-NP O\n","to TO B-VP O\n","boycott VB I-VP O\n","British JJ B-NP B-MISC\n","lamb NN I-NP O\n",". . O O\n","\\nAFTER : (Data/train.json)\n","        ]\n","      }\n","    ]\n","  },\n","  {\n","    \"id\":1,\n","    \"paragraphs\":[\n","      {\n","        \"sentences\":[\n","          {\n","            \"tokens\":[\n","              {\n","                \"orth\":\"EU\",\n","                \"tag\":\"NNP\",\n","                \"ner\":\"U-ORG\"\n","              },\n","              {\n","                \"orth\":\"rejects\",\n","                \"tag\":\"VBZ\",\n","                \"ner\":\"O\"\n","              },\n","              {\n","                \"orth\":\"German\",\n","                \"tag\":\"JJ\",\n","                \"ner\":\"U-MISC\"\n","              },\n","              {\n","                \"orth\":\"call\",\n","                \"tag\":\"NN\",\n","                \"ner\":\"O\"\n","              },\n","              {\n","                \"orth\":\"to\",\n","                \"tag\":\"TO\",\n","                \"ner\":\"O\"\n","              },\n","              {\n","                \"orth\":\"boycott\",\n","                \"tag\":\"VB\",\n","                \"ner\":\"O\"\n","              },\n","              {\n","                \"orth\":\"British\",\n","                \"tag\":\"JJ\",\n","                \"ner\":\"U-MISC\"\n","              },\n","              {\n","                \"orth\":\"lamb\",\n","                \"tag\":\"NN\",\n"]}]},{"cell_type":"markdown","metadata":{"id":"OUAVYAzg1pSQ"},"source":["## Training the NER model with Spacy (CLI)\n","\n","All the commandline options can be seen at: https://spacy.io/api/cli#train\n","We are training using the train program in spacy, for English (en), and the results are stored in a folder \n","called \"model\" (created while training). Our training file is in \"spacyNER_data/train.json\" and the validation file is at: \"spacyNER_data/valid.json\". \n","\n","-G stands for gpu option.\n","-p stands for pipeline, and it should be followed by a comma separated set of options - in this case, a tagger and an NER are being trained simultaneously"]},{"cell_type":"code","metadata":{"id":"wXjVXzQ41pSQ","executionInfo":{"status":"ok","timestamp":1635985037946,"user_tz":240,"elapsed":8,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}}},"source":["# %time !python3 -m spacy train en model spacyNER_data/train.json spacyNER_data/valid.json -G -p tagger,ner\n","#Wall time: 32min 29s"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hqh_JVPw4IVu","executionInfo":{"status":"ok","timestamp":1635987006034,"user_tz":240,"elapsed":1968095,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"c9236ba1-73be-47a7-cf2d-a38eb2f7ef77"},"source":["import time #33\n","start = time.time()\n","!python3 -m spacy train en model spacyNER_data/train.json spacyNER_data/valid.json -G -p tagger,ner\n","end = time.time()\n","print(\"Time taken for training is:{:.2f} hrs \".format((end-start)/3600.0))"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[38;5;2m✔ Created output directory: model\u001b[0m\n","Training pipeline: ['tagger', 'ner']\n","Starting with blank model 'en'\n","Counting training words (limit=0)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W022] Training a new part-of-speech tagger using a model with no lemmatization rules or data. This means that the trained model may not be able to lemmatize correctly. If this is intentional or the language you're using doesn't have lemmatization data, you can ignore this warning by setting SPACY_WARNING_IGNORE=W022. If this is surprising, make sure you have the spacy-lookups-data package installed.\n","  \"__main__\", mod_spec)\n","\n","Itn  Tag Loss    Tag %    NER Loss   NER P   NER R   NER F   Token %  CPU WPS\n","---  ---------  --------  ---------  ------  ------  ------  -------  -------\n","  1  31330.681    94.249  16686.563  82.790  82.413  82.601  100.000    11524\n","  2  16879.955    94.897   7739.659  86.715  85.796  86.253  100.000    11811\n","  3  13688.029    95.174   5244.834  87.638  86.974  87.305  100.000    11571\n","  4  11794.738    95.281   3891.267  88.215  87.933  88.074  100.000    11790\n","  5  10520.640    95.343   3137.274  88.446  88.118  88.282  100.000    11642\n","  6   9585.011    95.399   2486.873  88.686  88.253  88.469  100.000    11737\n","  7   8943.334    95.457   2304.852  88.563  87.967  88.264  100.000    11698\n","  8   8428.171    95.486   1948.291  88.650  88.068  88.358  100.000    11592\n","  9   8042.336    95.500   2002.403  88.727  88.085  88.405  100.000    11693\n"," 10   7539.648    95.523   1701.769  88.934  88.186  88.558  100.000    11772\n"," 11   7091.848    95.545   1618.559  89.043  88.354  88.697  100.000    11655\n"," 12   6789.537    95.562   1443.327  89.047  88.522  88.784  100.000    11525\n"," 13   6611.832    95.550   1485.776  89.113  88.573  88.842  100.000    11637\n"," 14   6227.981    95.580   1291.674  88.908  88.489  88.698  100.000    11739\n"," 15   6089.849    95.574   1219.881  88.660  88.287  88.473  100.000    11713\n"," 16   5790.468    95.587   1212.017  88.778  88.405  88.591  100.000    11561\n"," 17   5531.926    95.531   1111.443  88.842  88.438  88.640  100.000    11630\n"," 18   5297.408    95.611   1167.115  88.857  88.438  88.647  100.000    11624\n"," 19   5228.501    95.581   1122.893  88.857  88.438  88.647  100.000    11799\n"," 20   5051.102    95.611   1072.179  88.923  88.623  88.773  100.000    11739\n"," 21   4847.639    95.622    952.931  89.062  88.657  88.859  100.000    11775\n"," 22   4662.570    95.611    935.245  88.902  88.573  88.737  100.000    11835\n"," 23   4639.438    95.580    940.309  88.729  88.371  88.550  100.000    11774\n"," 24   4376.078    95.595    816.584  88.761  88.388  88.574  100.000    11719\n"," 25   4371.946    95.609    812.681  88.863  88.489  88.675  100.000    11714\n"," 26   4346.657    95.636    758.609  88.896  88.522  88.709  100.000    11780\n"," 27   4141.435    95.618    776.996  88.855  88.556  88.705  100.000    11733\n"," 28   4020.872    95.614    762.335  88.755  88.337  88.546  100.000    11652\n"," 29   4000.445    95.601    780.168  88.654  88.236  88.445  100.000    11797\n"," 30   3750.703    95.609    763.338  88.643  88.270  88.456  100.000    11725\n","\u001b[38;5;2m✔ Saved model to output directory\u001b[0m\n","model/model-final\n","\u001b[2K\u001b[38;5;2m✔ Created best model\u001b[0m\n","model/model-best\n","Time taken for training is:0.55 hrs \n"]}]},{"cell_type":"markdown","metadata":{"id":"8Hqnu22g1pSR"},"source":["Notice how the performance improves with each iteration!\n","## Evaluating the model with test data set (`spacyNER_data/test.json`)"]},{"cell_type":"markdown","metadata":{"id":"Zr6hKQnh1pSR"},"source":["### On Trained model (`model/model-best`)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RVNXEoBs1pSR","executionInfo":{"status":"ok","timestamp":1635987014450,"user_tz":240,"elapsed":8422,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"a7e36d21-34f8-4a44-de41-d511b7c967c1"},"source":["#create a folder to store the output and visualizations. \n","!mkdir result\n","!python3 -m spacy evaluate model/model-best spacyNER_data/test.json -dp result\n","# !python -m spacy evaluate model/model-final data/test.txt.json -dp result"],"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m\n","================================== Results ==================================\u001b[0m\n","\n","Time      4.13 s\n","Words     46666 \n","Words/s   11307 \n","TOK       100.00\n","POS       95.22 \n","UAS       0.00  \n","LAS       0.00  \n","NER P     81.68 \n","NER R     81.78 \n","NER F     81.73 \n","Textcat   0.00  \n","\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n","result\n"]}]},{"cell_type":"markdown","metadata":{"id":"Mo8Lk9mc1pSS"},"source":["a Visualization of the entity tagged test data can be seen in result/entities.html folder. "]},{"cell_type":"markdown","metadata":{"id":"i2S73yCM1pSS"},"source":["### On spacy's Pretrained NER model (`en`)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b8zX6UVz1pSS","executionInfo":{"status":"ok","timestamp":1635987026353,"user_tz":240,"elapsed":11906,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"8ffcbcfc-4c6e-4c84-a667-3a7681a6675e"},"source":["!mkdir pretrained_result\n","!python3 -m spacy evaluate en spacyNER_data/test.json -dp pretrained_result"],"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1m\n","================================== Results ==================================\u001b[0m\n","\n","Time      7.57 s\n","Words     46666 \n","Words/s   6168  \n","TOK       100.00\n","POS       86.21 \n","UAS       0.00  \n","LAS       0.00  \n","NER P     6.51  \n","NER R     9.17  \n","NER F     7.62  \n","Textcat   0.00  \n","\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","/usr/lib/python3.7/runpy.py:193: UserWarning: [W006] No entities to visualize found in Doc object. If this is surprising to you, make sure the Doc was processed using a model that supports named entity recognition, and check the `doc.ents` property manually if necessary.\n","  \"__main__\", mod_spec)\n","\u001b[38;5;2m✔ Generated 25 parses as HTML\u001b[0m\n","pretrained_result\n"]}]},{"cell_type":"markdown","metadata":{"id":"OWTVcm-I1pSS"},"source":["a Visualization of the entity tagged test data can be seen in pretrained_result/entities.html folder. "]}]}