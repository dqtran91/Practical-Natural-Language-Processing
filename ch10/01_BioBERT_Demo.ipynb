{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"01_BioBERT_Demo.ipynb","provenance":[{"file_id":"https://github.com/practical-nlp/practical-nlp-code/blob/master/Ch10/01_BioBERT_Demo.ipynb","timestamp":1636596708418}],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"dnX4TfNmTbeH"},"source":["In this notebook we demostrate how to use BioBERT , which is BERT pre trained on a huge corpus of medical data. We will demonstrate text classification. Tasks such as NER and slot filling can be easily performed by replacing the pre-trained model with the pre-trainied biobert.\n","<br><br>\n","Now to make use of the pre-trained bioBert model with the hugging face transformers library we need the model's weights to be in a form which pyTorch understands. The original model was trained using tensorflow so we need to convert the weights into pyTorch weights. We can then import and use it.\n"]},{"cell_type":"markdown","metadata":{"id":"mqXcmmf9katE"},"source":["First we will download the model from github repo of [BioBert](https://github.com/dmis-lab/biobert).<br> Huge shoutout to this [article](https://medium.com/@acpanjan/download-google-drive-files-using-wget-3c2c025a8b99) which helped in using wget to download the pre-trained model.\n","\n","Download the mtsamples.csv dataset which can be found [here](https://www.kaggle.com/tboyle10/medicaltranscriptions)"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nfXXyJm0hOBh","executionInfo":{"status":"ok","timestamp":1639102762360,"user_tz":300,"elapsed":53573,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"5be624fb-724c-4f8b-ea17-441a24802a60"},"source":["## To install only the requirements of this notebook, uncomment the lines below and run this cell\n","!pip install requests==2.23.0\n","!pip install pytorch-transformers==1.2.0\n","!pip install transformers==4.7.0\n","!pip install pandas==1.1.5\n","!pip install pytorch-pretrained-bert==0.6.2\n","!pip install pytorch-nlp==0.5.0\n","!pip install tensorflow #==1.14.0\n","!pip install torch #==1.10.0\n","!pip install scikit-learn==0.21.3\n","!pip install tqdm #==4.41.1\n","!pip install matplotlib==3.2.2\n","## To install the requirements for the entire chapter, uncomment the lines below and run this cell\n","# try:\n","#   import google.colab\n","#   !curl  https://raw.githubusercontent.com/practical-nlp/practical-nlp/master/Ch10/ch10-requirements.txt | xargs -n 1 -L 1 pip install\n","# except ModuleNotFoundError:\n","#   !pip install -r \"ch10-requirements.txt\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: requests==2.23.0 in /usr/local/lib/python3.7/dist-packages (2.23.0)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests==2.23.0) (2.10)\n","Collecting pytorch-transformers==1.2.0\n","  Downloading pytorch_transformers-1.2.0-py3-none-any.whl (176 kB)\n","\u001b[K     |████████████████████████████████| 176 kB 13.8 MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.19.5)\n","Collecting sacremoses\n","  Downloading sacremoses-0.0.46-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (4.62.3)\n","Requirement already satisfied: torch>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (1.10.0+cu111)\n","Collecting sentencepiece\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 37.0 MB/s \n","\u001b[?25hCollecting boto3\n","  Downloading boto3-1.20.23-py3-none-any.whl (131 kB)\n","\u001b[K     |████████████████████████████████| 131 kB 49.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-transformers==1.2.0) (2019.12.20)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.0.0->pytorch-transformers==1.2.0) (3.10.0.2)\n","Collecting jmespath<1.0.0,>=0.7.1\n","  Downloading jmespath-0.10.0-py2.py3-none-any.whl (24 kB)\n","Collecting botocore<1.24.0,>=1.23.23\n","  Downloading botocore-1.23.23-py3-none-any.whl (8.4 MB)\n","\u001b[K     |████████████████████████████████| 8.4 MB 34.0 MB/s \n","\u001b[?25hCollecting s3transfer<0.6.0,>=0.5.0\n","  Downloading s3transfer-0.5.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 7.8 MB/s \n","\u001b[?25hCollecting urllib3<1.27,>=1.25.4\n","  Downloading urllib3-1.26.7-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 53.7 MB/s \n","\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers==1.2.0) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3->pytorch-transformers==1.2.0) (1.15.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (2.10)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-transformers==1.2.0) (3.0.4)\n","  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n","\u001b[K     |████████████████████████████████| 127 kB 50.9 MB/s \n","\u001b[?25hRequirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->pytorch-transformers==1.2.0) (1.1.0)\n","Installing collected packages: urllib3, jmespath, botocore, s3transfer, sentencepiece, sacremoses, boto3, pytorch-transformers\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","Successfully installed boto3-1.20.23 botocore-1.23.23 jmespath-0.10.0 pytorch-transformers-1.2.0 s3transfer-0.5.0 sacremoses-0.0.46 sentencepiece-0.1.96 urllib3-1.25.11\n","Collecting transformers==4.7.0\n","  Downloading transformers-4.7.0-py3-none-any.whl (2.5 MB)\n","\u001b[K     |████████████████████████████████| 2.5 MB 11.1 MB/s \n","\u001b[?25hCollecting huggingface-hub==0.0.8\n","  Downloading huggingface_hub-0.0.8-py3-none-any.whl (34 kB)\n","Collecting tokenizers<0.11,>=0.10.1\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 46.9 MB/s \n","\u001b[?25hRequirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (0.0.46)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (1.19.5)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.62.3)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (21.3)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.13)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (4.8.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2.23.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (3.4.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.7.0) (2019.12.20)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.6.0)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.7.0) (3.10.0.2)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers==4.7.0) (3.0.6)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (1.25.11)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.7.0) (2021.10.8)\n","Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.15.0)\n","Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (7.1.2)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.7.0) (1.1.0)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.0.8 tokenizers-0.10.3 transformers-4.7.0\n","Requirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (2018.9)\n","Requirement already satisfied: numpy>=1.15.4 in /usr/local/lib/python3.7/dist-packages (from pandas==1.1.5) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas==1.1.5) (1.15.0)\n","Collecting pytorch-pretrained-bert==0.6.2\n","  Downloading pytorch_pretrained_bert-0.6.2-py3-none-any.whl (123 kB)\n","\u001b[K     |████████████████████████████████| 123 kB 11.5 MB/s \n","\u001b[?25hRequirement already satisfied: boto3 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (1.20.23)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (1.19.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (2.23.0)\n","Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (2019.12.20)\n","Requirement already satisfied: torch>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from pytorch-pretrained-bert==0.6.2) (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=0.4.1->pytorch-pretrained-bert==0.6.2) (3.10.0.2)\n","Requirement already satisfied: botocore<1.24.0,>=1.23.23 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2) (1.23.23)\n","Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2) (0.5.0)\n","Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/dist-packages (from boto3->pytorch-pretrained-bert==0.6.2) (0.10.0)\n","Requirement already satisfied: urllib3<1.27,>=1.25.4 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-pretrained-bert==0.6.2) (1.25.11)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.7/dist-packages (from botocore<1.24.0,>=1.23.23->boto3->pytorch-pretrained-bert==0.6.2) (2.8.2)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.24.0,>=1.23.23->boto3->pytorch-pretrained-bert==0.6.2) (1.15.0)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (3.0.4)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pytorch-pretrained-bert==0.6.2) (2021.10.8)\n","Installing collected packages: pytorch-pretrained-bert\n","Successfully installed pytorch-pretrained-bert-0.6.2\n","Collecting pytorch-nlp==0.5.0\n","  Downloading pytorch_nlp-0.5.0-py3-none-any.whl (90 kB)\n","\u001b[K     |████████████████████████████████| 90 kB 5.9 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp==0.5.0) (4.62.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from pytorch-nlp==0.5.0) (1.19.5)\n","Installing collected packages: pytorch-nlp\n","Successfully installed pytorch-nlp-0.5.0\n","Requirement already satisfied: tensorflow in /usr/local/lib/python3.7/dist-packages (2.7.0)\n","Requirement already satisfied: tensorflow-estimator<2.8,~=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.1.0)\n","Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.2.0)\n","Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.6.3)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.3.0)\n","Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.13.3)\n","Requirement already satisfied: keras<2.8,>=2.7.0rc0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: flatbuffers<3.0,>=1.12 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.0)\n","Requirement already satisfied: absl-py>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.12.0)\n","Requirement already satisfied: libclang>=9.0.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (12.0.0)\n","Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.19.5)\n","Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.42.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.15.0)\n","Requirement already satisfied: keras-preprocessing>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (1.1.2)\n","Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.17.3)\n","Requirement already satisfied: gast<0.5.0,>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.4.0)\n","Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.22.0)\n","Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (3.10.0.2)\n","Requirement already satisfied: wheel<1.0,>=0.32.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (0.37.0)\n","Requirement already satisfied: tensorboard~=2.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow) (2.7.0)\n","Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py>=2.9.0->tensorflow) (1.5.2)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.8.0)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (3.3.6)\n","Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (57.4.0)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (2.23.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard~=2.6->tensorflow) (1.0.1)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (4.8)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (1.3.0)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard~=2.6->tensorflow) (4.8.2)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.6->tensorflow) (3.6.0)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard~=2.6->tensorflow) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2021.10.8)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (1.25.11)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard~=2.6->tensorflow) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard~=2.6->tensorflow) (3.1.1)\n","Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n","Collecting scikit-learn==0.21.3\n","  Downloading scikit_learn-0.21.3-cp37-cp37m-manylinux1_x86_64.whl (6.7 MB)\n","\u001b[K     |████████████████████████████████| 6.7 MB 8.5 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.19.5)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.1.0)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn==0.21.3) (1.4.1)\n","Installing collected packages: scikit-learn\n","  Attempting uninstall: scikit-learn\n","    Found existing installation: scikit-learn 1.0.1\n","    Uninstalling scikit-learn-1.0.1:\n","      Successfully uninstalled scikit-learn-1.0.1\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","imbalanced-learn 0.8.1 requires scikit-learn>=0.24, but you have scikit-learn 0.21.3 which is incompatible.\u001b[0m\n","Successfully installed scikit-learn-0.21.3\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.62.3)\n","Requirement already satisfied: matplotlib==3.2.2 in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (3.0.6)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.3.2)\n","Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (2.8.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (0.11.0)\n","Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib==3.2.2) (1.19.5)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib==3.2.2) (1.15.0)\n"]}]},{"cell_type":"code","metadata":{"id":"6UX0jJep8NRG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639102768459,"user_tz":300,"elapsed":6113,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"0d27420b-9fd8-48e8-bb85-5a9e3531fe29"},"source":["#import packages\n","import requests\n","import tarfile\n","import os\n","import shutil\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=FutureWarning) #ignore warnings\n","\n","#define functions to download bioBert from google drive if wget doesn't work\n","def download_file_from_google_drive(id, destination):\n","    URL = \"https://docs.google.com/uc?export=download\"\n","\n","    session = requests.Session()\n","\n","    response = session.get(URL, params = { 'id' : id }, stream = True)\n","    token = get_confirm_token(response)\n","\n","    if token:\n","        params = { 'id' : id, 'confirm' : token }\n","        response = session.get(URL, params = params, stream = True)\n","\n","    save_response_content(response, destination)    \n","\n","def get_confirm_token(response):\n","    for key, value in response.cookies.items():\n","        if key.startswith('download_warning'):\n","            return value\n","\n","    return None\n","\n","def save_response_content(response, destination):\n","    CHUNK_SIZE = 32768\n","\n","    with open(destination, \"wb\") as f:\n","        for chunk in response.iter_content(CHUNK_SIZE):\n","            if chunk: # filter out keep-alive new chunks\n","                f.write(chunk)\n","\n","#download bioBert\n","try:\n","  import google.colab\n","  !wget --load-cookies /tmp/cookies.txt \"https://docs.google.com/uc?export=download&confirm=$(wget --quiet --save-cookies /tmp/cookies.txt --keep-session-cookies --no-check-certificate 'https://docs.google.com/uc?export=download&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD' -O- | sed -rn 's/.*confirm=([0-9A-Za-z_]+).*/\\1\\n/p')&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\" -O biobert_weights && rm -rf /tmp/cookies.txt\n","except ModuleNotFoundError:\n","  print(\"Using Google Drive\\n\")\n","  file_id = '1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD'\n","  destination = 'Data/biobert_weights'\n","  download_file_from_google_drive(file_id, destination)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--2021-12-10 02:19:22--  https://docs.google.com/uc?export=download&confirm=YW_E&id=1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD\n","Resolving docs.google.com (docs.google.com)... 142.251.5.102, 142.251.5.113, 142.251.5.139, ...\n","Connecting to docs.google.com (docs.google.com)|142.251.5.102|:443... connected.\n","HTTP request sent, awaiting response... 302 Moved Temporarily\n","Location: https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download [following]\n","--2021-12-10 02:19:22--  https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download\n","Resolving doc-0o-7c-docs.googleusercontent.com (doc-0o-7c-docs.googleusercontent.com)... 142.251.5.132, 2a00:1450:400c:c1b::84\n","Connecting to doc-0o-7c-docs.googleusercontent.com (doc-0o-7c-docs.googleusercontent.com)|142.251.5.132|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://docs.google.com/nonceSigner?nonce=q4a1gcmgppgi4&continue=https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=rtn8t1okfl2sqvsenpfon94egtlov9c7 [following]\n","--2021-12-10 02:19:22--  https://docs.google.com/nonceSigner?nonce=q4a1gcmgppgi4&continue=https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e%3Ddownload&hash=rtn8t1okfl2sqvsenpfon94egtlov9c7\n","Connecting to docs.google.com (docs.google.com)|142.251.5.102|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=q4a1gcmgppgi4&user=14421948226659402872Z&hash=hp1e6q4aausu5dv78jncs4857te9olf6 [following]\n","--2021-12-10 02:19:23--  https://doc-0o-7c-docs.googleusercontent.com/docs/securesc/t2v6fb7dj329qsp6ovuuld9mtc3dmfnr/i8pi775fdn2reo4ib04ute62el0gekb3/1639102725000/13799006341648886493/14421948226659402872Z/1R84voFKHfWV9xjzeLzWBbmY1uOMYpnyD?e=download&nonce=q4a1gcmgppgi4&user=14421948226659402872Z&hash=hp1e6q4aausu5dv78jncs4857te9olf6\n","Connecting to doc-0o-7c-docs.googleusercontent.com (doc-0o-7c-docs.googleusercontent.com)|142.251.5.132|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 401403346 (383M) [application/x-gzip]\n","Saving to: ‘biobert_weights’\n","\n","biobert_weights     100%[===================>] 382.81M   103MB/s    in 3.7s    \n","\n","2021-12-10 02:19:27 (103 MB/s) - ‘biobert_weights’ saved [401403346/401403346]\n","\n"]}]},{"cell_type":"markdown","metadata":{"id":"-4jlHKl--wOb"},"source":["Install the required libraries\n","\n","Unzip the file and convert the weights to a pytorch readable form. Stefan-it's solution to the issue [here](https://github.com/huggingface/transformers/issues/457) was extremely helpful."]},{"cell_type":"code","metadata":{"id":"6BXvJSsJka0Q"},"source":["try:\n","  print(\"part 1\")\n","  import google.colab\n","  !tar -xzf biobert_weights\n","  !ls biobert_v1.1_pubmed/\n","except ModuleNotFoundError:\n","  tar = tarfile.open('Data/biobert_weights','r')\n","  tar.extractall('Data/')\n","  tar.close()\n","  print(os.listdir('Data/biobert_v1.1_pubmed'))\n","\n","PATH = \".\"\n","try:\n","  import google.colab\n","  PATH = './'\n","except ModuleNotFoundError:\n","  PATH = 'Data/'\n","\n","try:\n","  print(\"part 2\")\n","  import google.colab\n","  !transformers-cli convert --model_type bert --tf_checkpoint biobert_v1.1_pubmed/model.ckpt-1000000 --config biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output biobert_v1.1_pubmed/pytorch_model.bin\n","except ModuleNotFoundError:\n","  !transformers-cli convert --model_type bert --tf_checkpoint Data/biobert_v1.1_pubmed/model.ckpt-1000000 --config Data/biobert_v1.1_pubmed/bert_config.json --pytorch_dump_output Data/biobert_v1.1_pubmed/pytorch_model.bin\n","\n","# try:\n","#   print(\"part 3\")\n","#   import google.colab \n","#   !ls biobert_v1.1_pubmed/\n","#   !mv biobert_v1.1_pubmed/bert_config.json biobert_v1.1_pubmed/config.json\n","#   !ls biobert_v1.1_pubmed/\n","# except ModuleNotFoundError:\n","#   print(os.listdir(PATH+'biobert_v1.1_pubmed'))\n","#   shutil.copy(PATH+'biobert_v1.1_pubmed/bert_config.json',PATH+'biobert_v1.1_pubmed/config.json')\n","#   print(os.listdir(PATH+'biobert_v1.1_pubmed'))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W8I3KD0a9ttJ","colab":{"base_uri":"https://localhost:8080/","height":414},"executionInfo":{"status":"ok","timestamp":1639102792107,"user_tz":300,"elapsed":1165,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"36a63aae-4a1f-43bc-c8e5-98e75d28b3e8"},"source":["# #importing the dataset\n","try:\n","  print('Using Wget')\n","  !wget https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch10/Data/mtsamples.csv -O mtsamples.csv \n","except ModuleNotFoundError:\n","  from google.colab import files\n","  uploaded = files.upload()\n","\n","import pandas as pd\n","df = pd.read_csv(PATH+'mtsamples.csv')\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Using Wget\n","--2021-12-10 02:19:50--  https://raw.githubusercontent.com/practical-nlp/practical-nlp-code/master/Ch10/Data/mtsamples.csv\n","Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n","Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 17007236 (16M) [text/plain]\n","Saving to: ‘mtsamples.csv’\n","\n","mtsamples.csv       100%[===================>]  16.22M  --.-KB/s    in 0.1s    \n","\n","2021-12-10 02:19:51 (153 MB/s) - ‘mtsamples.csv’ saved [17007236/17007236]\n","\n"]},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>description</th>\n","      <th>medical_specialty</th>\n","      <th>sample_name</th>\n","      <th>transcription</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A 23-year-old white female presents with comp...</td>\n","      <td>Allergy / Immunology</td>\n","      <td>Allergic Rhinitis</td>\n","      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n","      <td>allergy / immunology, allergic rhinitis, aller...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n","      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>Bariatrics</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n","      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2-D M-Mode. Doppler.</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 1</td>\n","      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n","      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2-D Echocardiogram</td>\n","      <td>Cardiovascular / Pulmonary</td>\n","      <td>2-D Echocardiogram - 2</td>\n","      <td>1.  The left ventricular cavity size and wall ...</td>\n","      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                           keywords\n","0           0  ...  allergy / immunology, allergic rhinitis, aller...\n","1           1  ...  bariatrics, laparoscopic gastric bypass, weigh...\n","2           2  ...  bariatrics, laparoscopic gastric bypass, heart...\n","3           3  ...  cardiovascular / pulmonary, 2-d m-mode, dopple...\n","4           4  ...  cardiovascular / pulmonary, 2-d, doppler, echo...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","metadata":{"id":"r2bl_Z96IPxN","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1639102792108,"user_tz":300,"elapsed":11,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"74ab2731-685c-4a31-edc4-0085ca3008b3"},"source":["print(df.shape)\n","df['medical_specialty'].value_counts()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(4999, 6)\n"]},{"output_type":"execute_result","data":{"text/plain":[" Surgery                          1103\n"," Consult - History and Phy.        516\n"," Cardiovascular / Pulmonary        372\n"," Orthopedic                        355\n"," Radiology                         273\n"," General Medicine                  259\n"," Gastroenterology                  230\n"," Neurology                         223\n"," SOAP / Chart / Progress Notes     166\n"," Obstetrics / Gynecology           160\n"," Urology                           158\n"," Discharge Summary                 108\n"," ENT - Otolaryngology               98\n"," Neurosurgery                       94\n"," Hematology - Oncology              90\n"," Ophthalmology                      83\n"," Nephrology                         81\n"," Emergency Room Reports             75\n"," Pediatrics - Neonatal              70\n"," Pain Management                    62\n"," Psychiatry / Psychology            53\n"," Office Notes                       51\n"," Podiatry                           47\n"," Dermatology                        29\n"," Dentistry                          27\n"," Cosmetic / Plastic Surgery         27\n"," Letters                            23\n"," Physical Medicine - Rehab          21\n"," Sleep Medicine                     20\n"," Endocrinology                      19\n"," Bariatrics                         18\n"," IME-QME-Work Comp etc.             16\n"," Chiropractic                       14\n"," Diets and Nutritions               10\n"," Rheumatology                       10\n"," Speech - Language                   9\n"," Autopsy                             8\n"," Lab Medicine - Pathology            8\n"," Allergy / Immunology                7\n"," Hospice - Palliative Care           6\n","Name: medical_specialty, dtype: int64"]},"metadata":{},"execution_count":5}]},{"cell_type":"markdown","metadata":{"id":"EnS0NLFRo-Ag"},"source":["lets try predicting the medical_speciality from the description\n","dataset which is highly imbalanced. Could remove ones less which are less than 5%. But as it is a demonstration let us just proceed with how the dataset it and look at the results\n","\n","From here it will be the same as the IMDB_sentiment_classification (ch4) notebook so we will not re-explain every step."]},{"cell_type":"code","metadata":{"id":"sy_R-uviOzxI","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1639102795863,"user_tz":300,"elapsed":3762,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"91894de0-5687-448b-ab41-73a4836542b4"},"source":["#importing a few necessary packages and setting the DATA directory\n","DATA_DIR=\".\"\n","import os\n","import numpy as np\n","import pickle\n","import tensorflow as tf\n"," \n","\n","# BERT imports\n","import torch\n","from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from sklearn.model_selection import train_test_split\n","from pytorch_pretrained_bert import BertTokenizer, BertConfig\n","from pytorch_pretrained_bert import BertAdam, BertForSequenceClassification\n","from tqdm import tqdm, trange\n","import pandas as pd\n","import io\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","\n","# specify GPU device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","if torch.cuda.is_available():\n","    n_gpu = torch.cuda.device_count()\n","    torch.cuda.get_device_name(0)\n","\n","from sklearn.preprocessing import LabelEncoder\n","\n","#Encoding medical_specialty\n","le = LabelEncoder()\n","df[\"medical_specialty\"] = le.fit_transform(df[\"medical_specialty\"])\n","df.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>description</th>\n","      <th>medical_specialty</th>\n","      <th>sample_name</th>\n","      <th>transcription</th>\n","      <th>keywords</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>A 23-year-old white female presents with comp...</td>\n","      <td>0</td>\n","      <td>Allergic Rhinitis</td>\n","      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n","      <td>allergy / immunology, allergic rhinitis, aller...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>2</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n","      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>Consult for laparoscopic gastric bypass.</td>\n","      <td>2</td>\n","      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n","      <td>HISTORY OF PRESENT ILLNESS: , I have seen ABC ...</td>\n","      <td>bariatrics, laparoscopic gastric bypass, heart...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>2-D M-Mode. Doppler.</td>\n","      <td>3</td>\n","      <td>2-D Echocardiogram - 1</td>\n","      <td>2-D M-MODE: , ,1.  Left atrial enlargement wit...</td>\n","      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>2-D Echocardiogram</td>\n","      <td>3</td>\n","      <td>2-D Echocardiogram - 2</td>\n","      <td>1.  The left ventricular cavity size and wall ...</td>\n","      <td>cardiovascular / pulmonary, 2-d, doppler, echo...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   Unnamed: 0  ...                                           keywords\n","0           0  ...  allergy / immunology, allergic rhinitis, aller...\n","1           1  ...  bariatrics, laparoscopic gastric bypass, weigh...\n","2           2  ...  bariatrics, laparoscopic gastric bypass, heart...\n","3           3  ...  cardiovascular / pulmonary, 2-d m-mode, dopple...\n","4           4  ...  cardiovascular / pulmonary, 2-d, doppler, echo...\n","\n","[5 rows x 6 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UGJEUBCrTQJE","executionInfo":{"status":"ok","timestamp":1639102798785,"user_tz":300,"elapsed":2938,"user":{"displayName":"dung tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj3Re1ab2ZEJ5Z3LszcblJMPlArZupaifrPJWH2RXM=s64","userId":"14954118940859551276"}},"outputId":"20725a45-a1e6-44cc-ff4d-7d611764d477"},"source":["description = list(df['description'])\n","# Tokenize with BERT tokenizer\n","tokenizer = BertTokenizer.from_pretrained(PATH+'biobert_v1.1_pubmed', do_lower_case=True)\n","\n","# Restricting the max size of Tokens to 512(BERT doest accept any more than this)\n","tokenized_texts = list(map(lambda t: ['[CLS]']+tokenizer.tokenize(t)+['[SEP]'] , description))\n","print (\"Tokenize the first sentence:\")\n","print (tokenized_texts[0])\n","\n","classes = list(df['medical_specialty'])"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Tokenize the first sentence:\n","['[CLS]', 'a', '23', '-', 'year', '-', 'old', 'white', 'female', 'presents', 'with', 'complaint', 'of', 'all', '##er', '##gies', '.', '[SEP]']\n"]}]},{"cell_type":"code","metadata":{"id":"VPgT9lnYUJ_V"},"source":["# Set the maximum sequence length. \n","MAX_LEN = 128\n","\n","# Pad our input tokens so that everything has a uniform length\n","input_ids = pad_sequences(list(map(tokenizer.convert_tokens_to_ids, tokenized_texts)),\n","                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n","input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]\n","input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n","\n","# Create attention masks\n","attention_masks = []\n","# Create a mask of 1s for each token followed by 0s for padding\n","for seq in input_ids:\n","    seq_mask = [float(i>0) for i in seq]\n","    attention_masks.append(seq_mask)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qauU8icyUngJ"},"source":["batch_size = 16\n","\n","# Use train_test_split to split our data into train and validation sets for training\n","train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, classes, \n","                                                            random_state=2020, test_size=0.1)\n","train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n","                                             random_state=2020, test_size=0.1)\n","                                             \n","# Convert all of our data into torch tensors, the required datatype for our model\n","train_inputs = torch.tensor(train_inputs)\n","validation_inputs = torch.tensor(validation_inputs)\n","train_labels = torch.tensor(train_labels)\n","validation_labels = torch.tensor(validation_labels)\n","train_masks = torch.tensor(train_masks)\n","validation_masks = torch.tensor(validation_masks)\n","\n","# Create an iterator of our data with torch DataLoader \n","train_data = TensorDataset(train_inputs, train_masks, train_labels)\n","train_sampler = RandomSampler(train_data)\n","train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n","validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n","validation_sampler = SequentialSampler(validation_data)\n","validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n","\n","#Loading pre trained BERT\n","#import model\n","from pytorch_transformers import BertModel\n","# model = BertModel.from_pretrained(PATH+'biobert_v1.1_pubmed')\n","model = BertForSequenceClassification.from_pretrained(PATH+'biobert_v1.1_pubmed', num_labels=40)#binary classification\n","\n","if torch.cuda.is_available():\n","    print(model.cuda())\n","else:\n","    print(model)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":235},"id":"RzGq3XXmU37D","executionInfo":{"status":"error","timestamp":1657026098347,"user_tz":240,"elapsed":27,"user":{"displayName":"dung tran","userId":"14954118940859551276"}},"outputId":"56ed1f02-a7b0-4121-fa0f-5f743738bdfa"},"source":["# BERT fine-tuning parameters\n","\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'gamma', 'beta']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n","     'weight_decay_rate': 0.0}\n","]\n","\n","optimizer = BertAdam(optimizer_grouped_parameters,\n","                     lr=2e-5,\n","                     warmup=.1)\n","\n","# Function to calculate the accuracy of our predictions vs labels\n","def flat_accuracy(preds, labels):\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","torch.cuda.empty_cache() \n","# Store our loss and accuracy for plotting\n","train_loss_set = []\n","# Number of training epochs \n","epochs = 2#4\n","\n","# BERT training loop\n","for _ in trange(epochs, desc=\"Epoch\"):  \n","  \n","  ## TRAINING\n","  \n","    # Set our model to training mode\n","    model.train()  \n","    # Tracking variables\n","    tr_loss = 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    # Train the data for one epoch\n","    for step, batch in enumerate(train_dataloader):\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Clear out the gradients (by default they accumulate)\n","        optimizer.zero_grad()\n","        # Forward pass\n","        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n","        train_loss_set.append(loss.item())    \n","        # Backward pass\n","        loss.backward()\n","        # Update parameters and take a step using the computed gradient\n","        optimizer.step()\n","        # Update tracking variables\n","        tr_loss += loss.item()\n","        nb_tr_examples += b_input_ids.size(0)\n","        nb_tr_steps += 1\n","    print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n","       \n","  ## VALIDATION\n","\n","    # Put model in evaluation mode\n","    model.eval()\n","    # Tracking variables \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_steps, nb_eval_examples = 0, 0\n","    # Evaluate data for one epoch\n","    for batch in validation_dataloader:\n","        # Add batch to GPU\n","        batch = tuple(t.to(device) for t in batch)\n","        # Unpack the inputs from our dataloader\n","        b_input_ids, b_input_mask, b_labels = batch\n","        # Telling the model not to compute or store gradients, saving memory and speeding up validation\n","        with torch.no_grad():\n","          # Forward pass, calculate logit predictions\n","            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)    \n","        # Move logits and labels to CPU\n","        logits = logits.detach().cpu().numpy()\n","        label_ids = b_labels.to('cpu').numpy()\n","        tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n","        eval_accuracy += tmp_eval_accuracy\n","        nb_eval_steps += 1\n","    print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))\n","\n","# plot training performance\n","plt.figure(figsize=(15,8))\n","plt.title(\"Training loss\")\n","plt.xlabel(\"Batch\")\n","plt.ylabel(\"Loss\")\n","plt.plot(train_loss_set)\n","plt.show()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-7d0e7e596281>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# BERT fine-tuning parameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mparam_optimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnamed_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mno_decay\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'bias'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'gamma'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'beta'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m optimizer_grouped_parameters = [\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}]},{"cell_type":"markdown","metadata":{"id":"iXXp4l9lenRu"},"source":["The model performs very bad in terms of accuracy. Pre-processing the data and hyper parameter tuning will help us to achieve better results."]}]}